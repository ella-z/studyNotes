# 聚焦爬虫与数据解析
## 聚焦爬虫
- 功能：爬取页面中指定的页面内容。
- 编码流程：
   1. 指定url
   2. 发起请求
   3. 获取响应数据
   4. 数据解析
   5. 持久化存储

## 数据解析
### 数据解析原理
- 解析的局部的文本内容都会在标签之间或者标签的属性中进行存储
- 分为两步：
   1. 进行指定标签的定位。
   2. 标签或者标签对应的属性中存储的数据值进行提取(解析)
### 数据解析分类
   - 数据解析一共有三类，分别是：
      1. 正则
      2. bs4
      3. xpath(重点)
### 正则数据解析
- 爬取网站的图片
   - 小栗子：
     ```
      🌰：
        import requests
        url = 'imgUrl'
        # content以二进制的形式获取数据
        reponse = requests.get(url).content
        # 二进制写入文件时，要使用b进行声明，因为只文本w方式写入时, 遇到\n自动替换成\r\n, 所以用二进制读取时, 显示\r\n的格式, 自然以文本读时, 会自动换行
        with open('猫咪.png', 'wb') as fp:
            fp.write(reponse)
     ```
- 爬取整个页面的照片
   - 小栗子：
    ```
      🌰：
      import requests
      # re模块时python独有的匹配字符串的模块
      import re
      url = 'https://sc.chinaz.com/tupian/xiaomaotupian.html'
      headers={
          'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'
      }
      # 获取页面数据
      image_html_text = requests.get(url=url,headers=headers).text
      # 正则匹配图片地址
      ex = '<div class="box picblock col3.*?<img.*?src2="(.*?)".*?'
      # 匹配图片
      #re.I	使匹配对大小写不敏感
      #re.L	做本地化识别（locale-aware）匹配
      #re.M	多行匹配，影响 ^ 和 $
      #re.S	使 . 匹配包括换行在内的所有字符
      #re.U	根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.
      #re.X	该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。
      img_src_list = re.findall(ex, image_html_text, re.S)

      for index,src in enumerate(img_src_list):
          reponse = requests.get(f'https:{src}').content
          with open(f'./catSet/{index}.png', 'wb') as fp:
              fp.write(reponse)
    ```

### bs4解析
- bs4解析是python独有的解析方式。
- 解析过程：
   - 原理：
      1. 实例化一个BeautifulSoup对象，并且将页面源码加载到该对象中。
      2. 通过调用BeautifulSoup对象中相关的属性或方法进行标签定位和数据提取。
   - 环境安装：
      - pip install bs4
      - pip install lxml

- 如何实例化BeautifulSoup对象
   1. 引入bs4(from bs4 import BeautifulSoup)
   2. 对象的实例化
      1. 将本地的html文档中的数据加载到该对象中
         ```
          🌰：
            fp = open('./test.html','r',encoding='utf-8')
            soup = BeautifulSoup(fp,'lxml')
         ```
      2. 将互联网上获取的页面源码加载到该对象中
         ```
         🌰：
            page_text = response.text
            soup = BeautifulSoup(page_text，'lxml')
         ```
   3. 提供的用于数据解析的方法和属性
      - soup.tagName:返回的是文档中第一次出现的tagName对应的标签
      - soup.find('tagName')：等同于soup.div
      - soup.find('tagName',class_='className'):返回className对应的tagName对应的标签。
      - soup.find_all('tagName',class_='className')：返回符合要求的所有标签
      - soup.select('某种选择器(id，class)')：返回所有符合条件的标签
         - 还可以进行层级select，例如：
         ```
          🌰：
          soup.select('.x>ul') //返回class为x的标签下所有的ul标签
         ```

### xpath解析
- 使用方法：
   1. 实例化一个etree的对象(from lxml import etree)，且需要将被解析的页面源码数据加载到该对象中。
      1. 将本地的html文档中的源码数据加载到etree对象中
         ```
            etree.parse(filePath)
         ```
      2. 可以将从互联网上获取的源码数据加载到该对象中
         ```
            etree.HTML('page_text')
         ```
   2. 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
      - xpath表达式：
         - /：表示的是从根节点开始定位，表示的是一个层级。
         - //：表示的是多个层级。可以表示从任意位置开始定位。
         - 属性定位：tag[@attrName="attrValue"]
            ```
            🌰：
               //div[@class="song"]
            ```
         - 索引定位：索引是从1开始的
            ```
            🌰：
               //div[@class="song"]/p[3] #定位到class='song'的div下第三个p标签
            ```
         - 取文本：text()，可以获取标签中的文本
            ```
            🌰：
               //div[@class="song"]/p[3]/text()
            ```
         - 取属性值：@attrName
             ```
            🌰：
               //div[@class="song"]/p[3]/@attrName
            ```
- 环境安装
   - 安装lxml解析器：pip install lxml
   
















