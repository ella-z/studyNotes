# CrawlSpider
- CrawlSpideræ˜¯ä¸€ä¸ªç±»ï¼Œæ˜¯Spiderçš„ä¸€ä¸ªå­ç±»ã€‚
- CrawlSpideræ˜¯å…¨ç«™æ•°æ®çˆ¬å–çš„æ–¹å¼ã€‚
- CrawlSpiderçš„ä½¿ç”¨ï¼š
   - åˆ›å»ºä¸€ä¸ªå·¥ç¨‹
   - åˆ›å»ºçˆ¬è™«æ–‡ä»¶è¦åŸºäºCrawlSpiderç±»ï¼š
      - scrapy genspider -t  crawl fileName url
      - é“¾æ¥æå–å™¨ï¼šæ ¹æ®æŒ‡å®šè§„åˆ™(allow="æ­£åˆ™")è¿›è¡ŒæŒ‡å®šé“¾æ¥çš„æå–ã€‚
      - è§„åˆ™è§£æå™¨ï¼šå°†é“¾æ¥æå–å™¨æå–åˆ°çš„é“¾æ¥è¿›è¡ŒæŒ‡å®šè§„åˆ™(callback)çš„è§£ææ“ä½œã€‚
      ```
      ğŸŒ°ï¼š
         # ç”Ÿæˆæ–‡ä»¶
         from scrapy.linkextractors import LinkExtractor
         from scrapy.spiders import CrawlSpider, Rule
         
          class CrSpider(CrawlSpider):
          name = 'cr'
          allowed_domains = ['www.test.com']
          start_urls = ['http://www.test.com/']

          rules = (
              # Ruleï¼šè§„åˆ™è§£æå™¨
              # Ruleçš„æ„é€ æ–¹æ³•ä¸­æœ‰ä¸‰ä¸ªå‚æ•°ï¼šLinkExtractorï¼šé“¾æ¥æå–å™¨ï¼Œcallbackï¼šå›è°ƒå‡½æ•°ï¼Œfollow=Trueï¼šå¯ä»¥å°†é“¾æ¥æå–å™¨ï¼Œ
              ç»§ç»­ä½œç”¨åˆ°é“¾æ¥æå–å™¨å–åˆ°çš„é“¾æ¥æ‰€å¯¹åº”çš„é¡µé¢ä¸­ã€‚
              Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
          )

          def parse_item(self, response):
              item = {}
              #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
              #item['name'] = response.xpath('//div[@id="name"]').get()
              #item['description'] = response.xpath('//div[@id="description"]').get()
              return item
      ```
